# src/main.py
"""
Integrated Analytics Dashboard
- Fleet TCO Calculator
- Global TBR Market Dashboard
- EV Tire Insight Analytics

"""

import streamlit as st
import pandas as pd
import numpy as np
import sqlite3
import logging
import re
import traceback
import os
from typing import Optional, Dict, Any, List, Tuple
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# External libraries (install required)
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    from nltk.sentiment import SentimentIntensityAnalyzer
    from sklearn.feature_extraction.text import TfidfVectorizer
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False

# Reddit API library
try:
    import praw
    from tqdm import tqdm
    REDDIT_AVAILABLE = True
except ImportError:
    REDDIT_AVAILABLE = False

# ========================================
# CONFIGURATION
# ========================================
class Config:
    """Application configuration constants"""
    APP_TITLE = "ğŸš€ Integrated Analytics Dashboard"
    VERSION = "1.0.0"

    # Data folder paths
    DATA_FOLDER = "data"
    FLEET_DATA_FILES = {
        "fuel": "fuel_prices.csv",
        "distance": "vehicle_distance.csv",
        "efficiency": "vehicle_efficiency.csv"
    }
    TBR_DATA_FILES = {
        "csv": "trade_data.csv",
        "excel": "tbr_market_data.xlsx",
        "sqlite": "tbr_market.db"
    }
    EV_DATA_FILES = {
        "reddit": "ev_tire_reddit_filtered.csv"
    }

    # Fleet TCO Config
    DEFAULT_TIRE_COUNT = 10
    DEFAULT_TIRE_COST = 250000
    DEFAULT_REPLACE_INTERVAL = 12000

    # TBR Market Config
    TOP_MARKETS_COUNT = 5
    MIN_YEAR = 1990
    MAX_YEAR = 2030

    # EV Tire Config
    EV_KEYWORDS = ['tire', 'tyre', 'regenerative braking', 'noise', 'wear']
    SENTIMENT_THRESHOLD_POS = 0.05
    SENTIMENT_THRESHOLD_NEG = -0.05

# ========================================
# UTILITY FUNCTIONS
# ========================================
def setup_logging():
    """Setup application logging"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def format_currency(value: float, currency: str = "KRW") -> str:
    """Format currency with appropriate scaling"""
    if pd.isna(value) or value == 0:
        return f"0 {currency}"

    abs_value = abs(value)
    sign = "-" if value < 0 else ""

    if currency == "USD":
        if abs_value >= 1e12:
            return f"{sign}${abs_value/1e12:,.1f}T"
        elif abs_value >= 1e9:
            return f"{sign}${abs_value/1e9:,.1f}B"
        elif abs_value >= 1e6:
            return f"{sign}${abs_value/1e6:,.1f}M"
        elif abs_value >= 1e3:
            return f"{sign}${abs_value/1e3:,.1f}K"
        else:
            return f"{sign}${abs_value:,.0f}"
    else:  # KRW
        if abs_value >= 1e8:
            return f"{sign}{abs_value/1e8:,.1f}ì–µì›"
        elif abs_value >= 1e4:
            return f"{sign}{abs_value/1e4:,.0f}ë§Œì›"
        else:
            return f"{sign}{abs_value:,.0f}ì›"

def ensure_data_folder():
    """Ensure data folder exists"""
    if not os.path.exists(Config.DATA_FOLDER):
        os.makedirs(Config.DATA_FOLDER)
        st.info(f"ğŸ“ ë°ì´í„° í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {Config.DATA_FOLDER}")

def get_file_path(filename: str) -> str:
    """Get full file path in data folder"""
    return os.path.join(Config.DATA_FOLDER, filename)

def check_file_exists(filename: str) -> bool:
    """Check if file exists in data folder"""
    return os.path.exists(get_file_path(filename))

# ========================================
# DATA PROCESSING FUNCTIONS
# ========================================
class DataProcessor:
    """Common data processing utilities"""

    @staticmethod
    def load_csv_safe(file_path: str, encoding: str = 'utf-8') -> pd.DataFrame:
        """Safely load CSV file with fallback encodings"""
        encodings = [encoding, 'utf-8-sig', 'cp949', 'euc-kr']

        for enc in encodings:
            try:
                return pd.read_csv(file_path, encoding=enc)
            except UnicodeDecodeError:
                continue
            except Exception as e:
                st.error(f"CSV ë¡œë“œ ì˜¤ë¥˜ ({enc}): {str(e)}")
                break

        st.error("ì§€ì›ë˜ëŠ” ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    @staticmethod
    def load_csv_from_upload(file, encoding: str = 'utf-8') -> pd.DataFrame:
        """Safely load CSV file from upload with fallback encodings"""
        encodings = [encoding, 'utf-8-sig', 'cp949', 'euc-kr']

        for enc in encodings:
            try:
                file.seek(0)  # Reset file pointer
                return pd.read_csv(file, encoding=enc)
            except UnicodeDecodeError:
                continue
            except Exception as e:
                st.error(f"CSV ë¡œë“œ ì˜¤ë¥˜ ({enc}): {str(e)}")
                break

        st.error("ì§€ì›ë˜ëŠ” ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    @staticmethod
    def load_excel_safe(file_path: str) -> pd.DataFrame:
        """Safely load Excel file"""
        try:
            return pd.read_excel(file_path)
        except Exception as e:
            st.error(f"Excel íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame()

    @staticmethod
    def load_excel_from_upload(file) -> pd.DataFrame:
        """Safely load Excel file from upload"""
        try:
            return pd.read_excel(file)
        except Exception as e:
            st.error(f"Excel íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame()

    @staticmethod
    def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> bool:
        """Validate if DataFrame has required columns"""
        missing_cols = [col for col in required_columns if col not in df.columns]
        if missing_cols:
            st.error(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_cols}")
            st.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
            return False
        return True

# ========================================
# FLEET TCO CALCULATOR
# ========================================
class FleetTCOCalculator:
    """Fleet Total Cost of Ownership Calculator"""

    def __init__(self):
        self.logger = setup_logging()
        self.use_default_data = True

    def load_distance_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Load and process distance data"""
        try:
            # Handle various column name patterns
            distance_col = None
            for col in df.columns:
                if 'ì£¼í–‰ê±°ë¦¬' in col or 'distance' in col.lower():
                    distance_col = col
                    break

            if not distance_col:
                st.error("ì£¼í–‰ê±°ë¦¬ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

            df_clean = df.copy()
            df_clean[distance_col] = pd.to_numeric(
                df_clean[distance_col].astype(str).str.strip(),
                errors='coerce'
            )

            # Standardize column names
            col_mapping = {}
            for col in df_clean.columns:
                if 'ì°¨ì¢…' in col:
                    col_mapping[col] = 'ì°¨ì¢…'
                elif 'ì—°ë£Œ' in col or 'ìœ í˜•' in col:
                    col_mapping[col] = 'ìœ í˜•'
                elif distance_col == col:
                    col_mapping[col] = 'í‰ê· ì£¼í–‰ê±°ë¦¬_km'

            df_clean = df_clean.rename(columns=col_mapping)
            return df_clean[['ì°¨ì¢…', 'ìœ í˜•', 'í‰ê· ì£¼í–‰ê±°ë¦¬_km']].dropna()

        except Exception as e:
            st.error(f"ì£¼í–‰ê±°ë¦¬ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame()

    def load_efficiency_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Load and process fuel efficiency data"""
        try:
            # Find efficiency column
            efficiency_col = None
            for col in df.columns:
                if 'ì—°ë¹„' in col or 'efficiency' in col.lower():
                    efficiency_col = col
                    break

            if not efficiency_col:
                st.error("ì—°ë¹„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

            df_clean = df.copy()
            df_clean[efficiency_col] = pd.to_numeric(
                df_clean[efficiency_col].astype(str).str.replace(',', '').str.strip(),
                errors='coerce'
            )

            # Find vehicle type column
            vehicle_col = None
            for col in df_clean.columns:
                if 'ì°¨ì¢…' in col:
                    vehicle_col = col
                    break

            if not vehicle_col:
                st.error("ì°¨ì¢… ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

            return (
                df_clean[[vehicle_col, efficiency_col]]
                .rename(columns={vehicle_col: 'ì°¨ì¢…', efficiency_col: 'ë³µí•©_ì—°ë¹„'})
                .groupby('ì°¨ì¢…', as_index=False)
                .mean()
            )

        except Exception as e:
            st.error(f"ì—°ë¹„ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame()

    def load_fuel_price_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Load and process fuel price data"""
        try:
            # Find date column
            date_col = None
            for col in df.columns:
                if 'êµ¬ë¶„' in col or 'date' in col.lower() or 'ë‚ ì§œ' in col:
                    date_col = col
                    break

            if not date_col:
                st.error("ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

            # Find fuel price column
            fuel_col = None
            for col in df.columns:
                if 'ê²½ìœ ' in col or 'diesel' in col.lower():
                    fuel_col = col
                    break

            if not fuel_col:
                st.error("ê²½ìœ  ê°€ê²© ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

            df_clean = df.copy()

            # Parse date - try multiple formats
            try:
                df_clean['date'] = pd.to_datetime(df_clean[date_col], format='%Yë…„%mì›”%dì¼')
            except:
                try:
                    df_clean['date'] = pd.to_datetime(df_clean[date_col])
                except:
                    st.error("ë‚ ì§œ í˜•ì‹ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return pd.DataFrame()

            # Clean price data
            df_clean[fuel_col] = pd.to_numeric(df_clean[fuel_col], errors='coerce')

            return df_clean[['date', fuel_col]].rename(columns={fuel_col: 'â„“ë‹¹_ê°€ê²©_ì›'}).dropna()

        except Exception as e:
            st.error(f"ì—°ë£Œ ê°€ê²© ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame()


    def reset_state(self):
        """ìƒíƒœ ì´ˆê¸°í™”"""
        self.use_default_data = False
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if 'fuel_df' in st.session_state:
            del st.session_state['fuel_df']
        if 'dist_df' in st.session_state:
            del st.session_state['dist_df']
        if 'eff_df' in st.session_state:
            del st.session_state['eff_df']

    def render(self):
        """Render Fleet TCO Calculator UI"""
        st.header("ğŸš› Fleet TCO Calculator")
        st.markdown("ì°¨ëŸ‰ ìš´ì˜ ì´ë¹„ìš©ì„ ê³„ì‚°í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.")

        # TCO ê³„ì‚° ì„¤ì •ì„ ìƒë‹¨ì— ë°°ì¹˜
        with st.expander("âš™ï¸ TCO ê³„ì‚° ì„¤ì •", expanded=True):
            col1, col2, col3 = st.columns(3)
            with col1:
                tire_count = st.number_input("íƒ€ì´ì–´ ìˆ˜", min_value=1, max_value=50, value=Config.DEFAULT_TIRE_COUNT)
            with col2:
                cost_per_tire = st.number_input("íƒ€ì´ì–´ ë‹¨ê°€(ì›)", min_value=0, value=Config.DEFAULT_TIRE_COST, step=10000)
            with col3:
                replace_interval = st.number_input("êµì²´ ì£¼ê¸°(km)", min_value=1000, value=Config.DEFAULT_REPLACE_INTERVAL, step=1000)

        # Check if default data files exist
        fuel_file_exists = check_file_exists(Config.FLEET_DATA_FILES["fuel"])
        dist_file_exists = check_file_exists(Config.FLEET_DATA_FILES["distance"])
        eff_file_exists = check_file_exists(Config.FLEET_DATA_FILES["efficiency"])

        all_files_exist = fuel_file_exists and dist_file_exists and eff_file_exists

        # Data source selection
        col1, col2 = st.columns([3, 1])
        with col1:
            if all_files_exist:
                st.success("âœ… ê¸°ë³¸ ë°ì´í„° íŒŒì¼ì´ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                self.use_default_data = True
            else:
                st.info("ğŸ“ ê¸°ë³¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
                self.use_default_data = False

        with col2:
            if st.button("ğŸ”„ ì´ˆê¸°í™”", key="fleet_reset", type="secondary"):
                self.reset_state()
                st.success("âœ… ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()

        # File uploads or use default data
        fuel_df = pd.DataFrame()
        dist_df = pd.DataFrame()
        eff_df = pd.DataFrame()

        if self.use_default_data and all_files_exist:
            # Load default data
            fuel_df = DataProcessor.load_csv_safe(get_file_path(Config.FLEET_DATA_FILES["fuel"]))
            dist_df = DataProcessor.load_csv_safe(get_file_path(Config.FLEET_DATA_FILES["distance"]))
            eff_df = DataProcessor.load_csv_safe(get_file_path(Config.FLEET_DATA_FILES["efficiency"]))

            st.info(f"ğŸ“Š ê¸°ë³¸ ë°ì´í„° ì‚¬ìš© ì¤‘: {Config.FLEET_DATA_FILES}")

        else:
            # File upload interface
            st.subheader("ğŸ“ ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ")
            col1, col2, col3 = st.columns(3)

            with col1:
                st.markdown("**ì£¼ìœ ì†Œ í‰ê·  ê°€ê²© ë°ì´í„°**")
                st.caption("ì˜ˆ: fuel_prices.csv")
                fuel_file = st.file_uploader("íŒŒì¼ ì„ íƒ", type=['csv'], key='fuel_file')
                if fuel_file:
                    fuel_df = DataProcessor.load_csv_from_upload(fuel_file)

            with col2:
                st.markdown("**ì°¨ëŸ‰ ì£¼í–‰ê±°ë¦¬ ë°ì´í„°**")
                st.caption("ì˜ˆ: vehicle_distance.csv")
                dist_file = st.file_uploader("íŒŒì¼ ì„ íƒ", type=['csv'], key='dist_file')
                if dist_file:
                    dist_df = DataProcessor.load_csv_from_upload(dist_file)

            with col3:
                st.markdown("**ì°¨ëŸ‰ ì—°ë¹„ ë°ì´í„°**")
                st.caption("ì˜ˆ: vehicle_efficiency.csv")
                eff_file = st.file_uploader("íŒŒì¼ ì„ íƒ", type=['csv'], key='eff_file')
                if eff_file:
                    eff_df = DataProcessor.load_csv_from_upload(eff_file)

        # TCO ê³„ì‚° ë° ê²°ê³¼ í‘œì‹œ
        if not fuel_df.empty and not dist_df.empty and not eff_df.empty:
            try:
                # Process data
                dist_data = self.load_distance_data(dist_df)
                eff_data = self.load_efficiency_data(eff_df)
                fuel_data = self.load_fuel_price_data(fuel_df)

                if dist_data.empty or eff_data.empty or fuel_data.empty:
                    st.error("ë°ì´í„° ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    return

                # Merge and calculate
                merged_data = pd.merge(dist_data, eff_data, on='ì°¨ì¢…', how='inner')

                # Filter for trucks and diesel - .loc ì‚¬ìš©ìœ¼ë¡œ ìˆ˜ì •
                mask = (merged_data['ì°¨ì¢…'].str.contains('í™”ë¬¼', na=False)) & \
                       (merged_data['ìœ í˜•'].str.contains('ê²½ìœ ', na=False))

                truck_data = merged_data.loc[mask].copy()  # ëª…ì‹œì ìœ¼ë¡œ .copy() ì‚¬ìš©

                if truck_data.empty:
                    st.warning("í™”ë¬¼ì°¨ + ê²½ìœ  ì¡°í•© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.")
                    truck_data = merged_data.iloc[:1].copy()  # .iloc ì‚¬ìš©

                # .locì„ ì‚¬ìš©í•´ì„œ ìƒˆ ì»¬ëŸ¼ ì¶”ê°€
                truck_data.loc[:, 'ì¼ì¼ì—°ë£Œì†Œë¹„ëŸ‰_â„“'] = truck_data['í‰ê· ì£¼í–‰ê±°ë¦¬_km'] / truck_data['ë³µí•©_ì—°ë¹„']

                # Expand data with fuel prices
                truck_data.loc[:, 'key'] = 1
                fuel_data_copy = fuel_data.copy()
                fuel_data_copy.loc[:, 'key'] = 1

                expanded_data = pd.merge(truck_data, fuel_data_copy, on='key').drop('key', axis=1)

                # Calculate costs - .loc ì‚¬ìš©
                expanded_data.loc[:, 'ì¼ì¼ì—°ë£Œë¹„_ì›'] = \
                    expanded_data['ì¼ì¼ì—°ë£Œì†Œë¹„ëŸ‰_â„“'] * expanded_data['â„“ë‹¹_ê°€ê²©_ì›']

                expanded_data.loc[:, 'year_month'] = expanded_data['date'].dt.to_period('M')


                monthly_cost = (
                    expanded_data
                    .groupby('year_month', as_index=False)['ì¼ì¼ì—°ë£Œë¹„_ì›']
                    .sum()
                    .rename(columns={'ì¼ì¼ì—°ë£Œë¹„_ì›': 'ì›”ê°„ì—°ë£Œë¹„_ì›'})
                )

                # Calculate annual TCO
                avg_km = truck_data['í‰ê· ì£¼í–‰ê±°ë¦¬_km'].iloc[0]
                annual_km = avg_km * 365
                annual_fuel = monthly_cost['ì›”ê°„ì—°ë£Œë¹„_ì›'].sum()
                annual_tire = (annual_km / replace_interval) * tire_count * cost_per_tire
                annual_tco = annual_fuel + annual_tire

                # Display results
                st.subheader("ğŸ“Š ì—°ê°„ ë¹„ìš© ìš”ì•½")
                col1, col2, col3 = st.columns(3)

                with col1:
                    st.metric("ì—°ê°„ ì—°ë£Œë¹„", format_currency(annual_fuel))
                with col2:
                    st.metric("ì—°ê°„ íƒ€ì´ì–´ë¹„ìš©", format_currency(annual_tire))
                with col3:
                    st.metric("ì—°ê°„ TCO", format_currency(annual_tco))

                # Charts
                st.subheader("ğŸ“ˆ ì›”ê°„ ì—°ë£Œë¹„ ì¶”ì´")
                monthly_chart_data = monthly_cost.copy()
                monthly_chart_data['date'] = monthly_chart_data['year_month'].dt.to_timestamp()

                fig = px.line(
                    monthly_chart_data,
                    x='date',
                    y='ì›”ê°„ì—°ë£Œë¹„_ì›',
                    title="ì›”ê°„ ì—°ë£Œë¹„ ì¶”ì´"
                )
                fig.update_layout(xaxis_title="ë‚ ì§œ", yaxis_title="ì—°ë£Œë¹„ (ì›)")
                st.plotly_chart(fig, use_container_width=True)

                # Fuel price trend
                st.subheader("â›½ ì—°ë£Œ ê°€ê²© ì¶”ì´")
                fig2 = px.line(
                    fuel_data,
                    x='date',
                    y='â„“ë‹¹_ê°€ê²©_ì›',
                    title="ì—°ë£Œ ê°€ê²© ì¶”ì´"
                )
                fig2.update_layout(xaxis_title="ë‚ ì§œ", yaxis_title="ê°€ê²© (ì›/â„“)")
                st.plotly_chart(fig2, use_container_width=True)

                # Cost breakdown pie chart
                st.subheader("ğŸ’° ì—°ê°„ ë¹„ìš© êµ¬ì„±")
                cost_data = pd.DataFrame({
                    'Category': ['ì—°ë£Œë¹„', 'íƒ€ì´ì–´ë¹„ìš©'],
                    'Amount': [annual_fuel, annual_tire]
                })

                fig3 = px.pie(
                    cost_data,
                    values='Amount',
                    names='Category',
                    title="ì—°ê°„ ë¹„ìš© êµ¬ì„±ë¹„"
                )
                st.plotly_chart(fig3, use_container_width=True)

            except Exception as e:
                st.error(f"TCO ê³„ì‚° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                st.text(traceback.format_exc())
        else:
            if not self.use_default_data:
                st.info("ğŸ“ CSV íŒŒì¼ 3ê°œë¥¼ ëª¨ë‘ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

# ========================================
# TBR MARKET DASHBOARD
# ========================================
# src/tbr_market_dashboard.py (ìˆ˜ì •ëœ ë¶€ë¶„)

class TBRMarketDashboard:
    """TBR Market Analysis Dashboard"""

    def __init__(self):
        self.logger = setup_logging()
        self.use_default_data = True
        self.column_mappings = {
            "country": ["reporteriso", "reporterISO", "reporter_iso", "country", "reporter"],
            "year": ["refmonth", "refMonth", "ref_month", "period", "year"],
            "flow": ["flowcode", "flowCode", "flow_code", "flowdesc", "flow"],
            "value": ["cifvalue", "fobvalue", "primaryvalue", "tradevalue", "value"]
        }
        self.flow_mappings = {
            'M': 'Import', 'X': 'Export', 1: 'Import', 2: 'Export',
            '1': 'Import', '2': 'Export', 'I': 'Import', 'E': 'Export'
        }

    def get_database_info(self, db_path: str) -> Dict[str, Any]:
        """Get database structure information with detailed debugging"""
        try:
            # Check if file exists
            if not os.path.exists(db_path):
                st.error(f"ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {db_path}")
                return {'tables': [], 'table_info': {}}

            # Check file size
            file_size = os.path.getsize(db_path)

            if file_size == 0:
                st.error("ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return {'tables': [], 'table_info': {}}

            with sqlite3.connect(db_path) as conn:
                # Get all tables
                tables_query = "SELECT name, type FROM sqlite_master WHERE type IN ('table', 'view') ORDER BY name;"

                try:
                    tables_df = pd.read_sql(tables_query, conn)
                    tables = tables_df['name'].tolist()
                except Exception as e:
                    # Try alternative approach
                    try:
                        cursor = conn.cursor()
                        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
                        results = cursor.fetchall()
                        tables = [row[0] for row in results]
                    except Exception as e2:
                        return {'tables': [], 'table_info': {}}

                if not tables:
                    return {'tables': [], 'table_info': {}}

                # Get columns for each table
                table_info = {}
                for table in tables:
                    try:
                        columns_query = f"PRAGMA table_info([{table}]);"
                        columns_df = pd.read_sql(columns_query, conn)

                        count_query = f"SELECT COUNT(*) as count FROM [{table}];"
                        count_result = pd.read_sql(count_query, conn)

                        table_info[table] = {
                            'columns': columns_df['name'].tolist(),
                            'row_count': count_result['count'].iloc[0]
                        }

                    except Exception as e:
                        table_info[table] = {
                            'columns': [],
                            'row_count': 0
                        }

                return {
                    'tables': tables,
                    'table_info': table_info
                }

        except sqlite3.DatabaseError as e:
            st.error(f"SQLite ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
            return {'tables': [], 'table_info': {}}
        except Exception as e:
            st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return {'tables': [], 'table_info': {}}

    def auto_load_best_table(self, db_path: str) -> Tuple[pd.DataFrame, str]:
        """ìë™ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ í…Œì´ë¸”ì„ ì°¾ì•„ì„œ ë¡œë“œ"""
        try:
            with sqlite3.connect(db_path) as conn:
                # Get database info
                db_info = self.get_database_info(db_path)

                if not db_info['tables']:
                    return pd.DataFrame(), ""

                # í…Œì´ë¸” ìš°ì„ ìˆœìœ„ ê²°ì •
                best_table = None
                max_score = 0

                for table_name, info in db_info['table_info'].items():
                    score = 0

                    # 1. ë°ì´í„° í–‰ ìˆ˜ (ë§ì„ìˆ˜ë¡ ì¢‹ìŒ)
                    score += min(info['row_count'] / 10000, 5)  # ìµœëŒ€ 5ì 

                    # 2. í…Œì´ë¸” ì´ë¦„ ì ìˆ˜ (trade ê´€ë ¨ í‚¤ì›Œë“œ)
                    name_keywords = ['trade', 'data', 'main', 'market', 'export', 'import']
                    for keyword in name_keywords:
                        if keyword in table_name.lower():
                            score += 3

                    # 3. ì»¬ëŸ¼ ë§¤ì¹­ ì ìˆ˜ (í•„ìš”í•œ ì»¬ëŸ¼ë“¤ì´ ìˆëŠ”ì§€)
                    required_columns = ['country', 'year', 'flow', 'value']
                    column_matches = 0

                    for req_col, candidates in self.column_mappings.items():
                        for candidate in candidates:
                            if candidate in [col.lower() for col in info['columns']]:
                                column_matches += 1
                                break

                    score += column_matches * 2  # ì»¬ëŸ¼ ë§¤ì¹˜ë‹¹ 2ì 

                    if score > max_score:
                        max_score = score
                        best_table = table_name

                if best_table:
                    # ì„ íƒëœ í…Œì´ë¸” ë¡œë“œ
                    query = f"SELECT * FROM [{best_table}]"
                    df = pd.read_sql(query, conn)

                    return df, best_table

                return pd.DataFrame(), ""

        except Exception as e:
            st.error(f"ìë™ í…Œì´ë¸” ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame(), ""

    def load_data_from_database(self, db_path: str, auto_load: bool = True) -> pd.DataFrame:
        """Load data from database with optional auto-loading"""
        try:
            with sqlite3.connect(db_path) as conn:
                # Get database info
                db_info = self.get_database_info(db_path)

                if not db_info['tables']:
                    st.error("ë°ì´í„°ë² ì´ìŠ¤ì— í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.")
                    return pd.DataFrame()

                # ìë™ ë¡œë“œ ëª¨ë“œ
                if auto_load:
                    df_auto, best_table = self.auto_load_best_table(db_path)

                    if not df_auto.empty:
                        st.success(f"âœ… ìë™ìœ¼ë¡œ ìµœì  í…Œì´ë¸” '{best_table}' ë¡œë“œ ì™„ë£Œ ({len(df_auto):,}í–‰)")

                        # ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡° ì •ë³´ (ì ‘ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ)
                        with st.expander("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡° ì •ë³´"):
                            for table_name, info in db_info['table_info'].items():
                                selected_indicator = "ğŸ¯ " if table_name == best_table else "ğŸ“‹ "
                                st.markdown(f"**{selected_indicator}{table_name}** ({info['row_count']:,}í–‰)")
                                st.caption(f"ì»¬ëŸ¼: {', '.join(info['columns'][:5])}{'...' if len(info['columns']) > 5 else ''}")

                        # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                        with st.expander("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", expanded=True):
                            col1, col2 = st.columns([3, 1])
                            with col1:
                                st.dataframe(df_auto.head(10))
                            with col2:
                                st.metric("ì´ í–‰ ìˆ˜", f"{len(df_auto):,}")
                                st.metric("ì´ ì»¬ëŸ¼ ìˆ˜", len(df_auto.columns))

                        return df_auto

                # ìˆ˜ë™ ì„ íƒ ëª¨ë“œ (ìë™ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë˜ëŠ” ì‚¬ìš©ìê°€ ë‹¤ë¥¸ í…Œì´ë¸” ì„ íƒ ì‹œ)
                st.subheader("ğŸ—„ï¸ í…Œì´ë¸” ìˆ˜ë™ ì„ íƒ")

                for table_name, info in db_info['table_info'].items():
                    with st.expander(f"ğŸ“‹ í…Œì´ë¸”: {table_name} ({info['row_count']:,}í–‰)"):
                        st.write("**ì»¬ëŸ¼:**", ", ".join(info['columns']))

                # í…Œì´ë¸” ì„ íƒ
                common_names = ['trade_data', 'trades', 'data', 'main', 'export_data', 'import_data']
                selected_table = None

                for common_name in common_names:
                    if common_name in db_info['tables']:
                        selected_table = common_name
                        break

                if selected_table is None:
                    selected_table = db_info['tables'][0]

                selected_table = st.selectbox(
                    "ë¶„ì„í•  í…Œì´ë¸” ì„ íƒ",
                    options=db_info['tables'],
                    index=db_info['tables'].index(selected_table) if selected_table in db_info['tables'] else 0
                )

                if st.button("ì„ íƒëœ í…Œì´ë¸” ë¡œë“œ"):
                    query = f"SELECT * FROM [{selected_table}]"
                    df = pd.read_sql(query, conn)
                    st.success(f"âœ… í…Œì´ë¸” '{selected_table}'ì—ì„œ {len(df):,}í–‰ ë¡œë“œ")

                    with st.expander("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                        st.dataframe(df.head(10))

                    return df

                return pd.DataFrame()

        except Exception as e:
            st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame()

    def detect_columns(self, df: pd.DataFrame) -> Dict[str, str]:
        """Intelligent column detection"""
        column_map = {}

        for target_col, candidates in self.column_mappings.items():
            for candidate in candidates:
                if candidate in df.columns:
                    column_map[target_col] = candidate
                    break

        return column_map

    def process_trade_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process trade data with intelligent column mapping"""
        try:
            df_work = df.copy()
            df_work.columns = [c.lower().strip() for c in df_work.columns]

            column_map = self.detect_columns(df_work)

            if len(column_map) < 4:
                missing = set(['country', 'year', 'flow', 'value']) - set(column_map.keys())
                st.error(f"í•„ìˆ˜ ì»¬ëŸ¼ ë§¤í•‘ ì‹¤íŒ¨: {missing}")
                st.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df_work.columns)}")
                return pd.DataFrame()

            # Apply mapping
            rename_dict = {v: k for k, v in column_map.items()}
            df_clean = df_work.rename(columns=rename_dict)[['country', 'year', 'flow', 'value']].copy()

            # Clean data
            df_clean['country'] = df_clean['country'].astype(str).str.strip().str.upper()
            df_clean['value'] = pd.to_numeric(df_clean['value'], errors='coerce')

            # Extract year from various formats
            def extract_year(val):
                try:
                    val_str = str(val)
                    if len(val_str) == 6 and val_str.isdigit():
                        return int(val_str[:4])
                    return int(float(val))
                except:
                    return None

            df_clean['year'] = df_clean['year'].apply(extract_year)

            # Map flow values
            def map_flow(val):
                if val in self.flow_mappings:
                    return self.flow_mappings[val]
                return str(val).title()

            df_clean['flow'] = df_clean['flow'].apply(map_flow)

            # Filter valid data
            df_clean = df_clean.dropna(subset=['country', 'year', 'flow', 'value'])
            df_clean = df_clean[
                (df_clean['year'] >= Config.MIN_YEAR) &
                (df_clean['year'] <= Config.MAX_YEAR) &
                (df_clean['value'] >= 0)
                ]

            return df_clean

        except Exception as e:
            st.error(f"ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame()

    def render_tableau_dashboard(self):
        """Render Tableau dashboard when using SQLite"""
        st.subheader("ğŸ“Š Tableau ëŒ€ì‹œë³´ë“œ")
        st.markdown("SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œ Tableau ì‹œê°í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")

        tableau_html = """
       <div class='tableauPlaceholder' id='viz1748617557024' style='position: relative'>
           <noscript>
               <a href='#'>
                   <img alt='ëŒ€ì‹œë³´ë“œ 1 ' src='https://public.tableau.com/static/images/Gl/GlobalTBRMarketDashboard/1/1_rss.png' style='border: none' />
               </a>
           </noscript>
           <object class='tableauViz' style='display:none;'>
               <param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' />
               <param name='embed_code_version' value='3' />
               <param name='site_root' value='' />
               <param name='name' value='GlobalTBRMarketDashboard&#47;1' />
               <param name='tabs' value='no' />
               <param name='toolbar' value='yes' />
               <param name='static_image' value='https://public.tableau.com/static/images/Gl/GlobalTBRMarketDashboard/1/1.png' />
               <param name='animate_transition' value='yes' />
               <param name='display_static_image' value='yes' />
               <param name='display_spinner' value='yes' />
               <param name='display_overlay' value='yes' />
               <param name='display_count' value='yes' />
               <param name='language' value='ko-KR' />
               <param name='filter' value='publish=yes' />
           </object>
       </div>
       <script type='text/javascript'>
           var divElement = document.getElementById('viz1748617557024');
           var vizElement = divElement.getElementsByTagName('object')[0];
           if (divElement.offsetWidth > 800) {
               vizElement.style.minWidth='1000px';
               vizElement.style.maxWidth='100%';
               vizElement.style.minHeight='1500px';
               vizElement.style.maxHeight=(divElement.offsetWidth*0.75)+'px';
           } else if (divElement.offsetWidth > 500) {
               vizElement.style.minWidth='420px';
               vizElement.style.maxWidth='100%';
               vizElement.style.minHeight='827px';
               vizElement.style.maxHeight=(divElement.offsetWidth*0.75)+'px';
           } else {
               vizElement.style.width='100%';
               vizElement.style.height='927px';
           }
           var scriptElement = document.createElement('script');
           scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';
           vizElement.parentNode.insertBefore(scriptElement, vizElement);
       </script>
       """

        st.components.v1.html(tableau_html, height=1600)

    def reset_state(self):
        """ìƒíƒœ ì´ˆê¸°í™”"""
        self.use_default_data = False
        if 'tbr_df' in st.session_state:
            del st.session_state['tbr_df']

    def render(self):
        """Render TBR Market Dashboard UI (ìˆ˜ì •ëœ ë²„ì „)"""
        st.header("ğŸŒ Global TBR Market Dashboard")
        st.markdown("ê¸€ë¡œë²Œ TBR ì‹œì¥ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

        # Check if default data files exist
        sqlite_file_exists = check_file_exists(Config.TBR_DATA_FILES["sqlite"])
        csv_file_exists = check_file_exists(Config.TBR_DATA_FILES["csv"])
        excel_file_exists = check_file_exists(Config.TBR_DATA_FILES["excel"])

        any_file_exists = sqlite_file_exists or csv_file_exists or excel_file_exists

        # Data source selection
        col1, col2 = st.columns([3, 1])
        with col1:
            if any_file_exists:
                available_files = []
                if sqlite_file_exists:
                    available_files.append("SQLite DB")
                if csv_file_exists:
                    available_files.append("CSV")
                if excel_file_exists:
                    available_files.append("Excel")

                st.success(f"âœ… ê¸°ë³¸ ë°ì´í„° íŒŒì¼ ì¤€ë¹„ë¨: {', '.join(available_files)}")
                self.use_default_data = True
            else:
                st.info("ğŸ“ ê¸°ë³¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
                self.use_default_data = False

        with col2:
            if st.button("ğŸ”„ ì´ˆê¸°í™”", key="tbr_reset"):
                self.use_default_data = False
                st.rerun()

        # ê¸°ë³¸ì ìœ¼ë¡œ SQLite ë°ì´í„°ë² ì´ìŠ¤ ìš°ì„  ì„ íƒ
        df_trade = pd.DataFrame()

        # SQLite íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ë¡œë“œ ì‹œë„
        if sqlite_file_exists:
            st.info("ğŸ”„ SQLite ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìë™ ë¡œë“œ ì¤‘...")

            try:
                # ìë™ ë¡œë“œ ì‹œë„
                df_raw = self.load_data_from_database(
                    get_file_path(Config.TBR_DATA_FILES["sqlite"]),
                    auto_load=True
                )

                # 1. ìë™ ë¡œë“œ ì„±ê³µ ì‹œ Tableau ëŒ€ì‹œë³´ë“œ í‘œì‹œ ë¶€ë¶„
                if not df_raw.empty:
                    df_trade = self.process_trade_data(df_raw)
                    if not df_trade.empty:
                        st.success("âœ… SQLite ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìë™ ë¡œë“œ ì™„ë£Œ!")

                        # Tableau ëŒ€ì‹œë³´ë“œ í‘œì‹œ (ê¸°ë³¸ì ìœ¼ë¡œ í¼ì³ì§)
                        with st.expander("ğŸ“Š Tableau ëŒ€ì‹œë³´ë“œ ë³´ê¸°", expanded=True):
                            self.render_tableau_dashboard()
                    else:
                        st.warning("âš ï¸ ë°ì´í„° ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ë¥¸ ì˜µì…˜ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

            except Exception as e:
                st.error(f"âŒ ìë™ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                st.info("ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")

        # ìë™ ë¡œë“œê°€ ì‹¤íŒ¨í–ˆê±°ë‚˜ SQLite íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì„ íƒ ì˜µì…˜ í‘œì‹œ
        if df_trade.empty:
            st.markdown("---")
            st.subheader("ğŸ“ ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ")

            if self.use_default_data and any_file_exists:
                data_source = st.radio(
                    "ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ",
                    ["SQLite ë°ì´í„°ë² ì´ìŠ¤", "ê¸°ë³¸ CSV/Excel", "íŒŒì¼ ì—…ë¡œë“œ"],
                    key="tbr_data_source"
                )
            else:
                data_source = st.radio(
                    "ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ",
                    ["SQLite ë°ì´í„°ë² ì´ìŠ¤", "íŒŒì¼ ì—…ë¡œë“œ"],
                    key="tbr_data_source_upload"
                )

            if data_source == "SQLite ë°ì´í„°ë² ì´ìŠ¤":
                db_path = st.text_input("ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ", value=get_file_path(Config.TBR_DATA_FILES["sqlite"]))

                if st.button("ğŸ”„ ìˆ˜ë™ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ"):
                    df_raw = self.load_data_from_database(db_path, auto_load=False)
                    if not df_raw.empty:
                        df_trade = self.process_trade_data(df_raw)
                        if not df_trade.empty:
                            st.success("âœ… SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
                            # Tableau ëŒ€ì‹œë³´ë“œ í‘œì‹œ (ê¸°ë³¸ì ìœ¼ë¡œ í¼ì³ì§)
                            with st.expander("ğŸ“Š Tableau ëŒ€ì‹œë³´ë“œ ë³´ê¸°", expanded=True):
                                self.render_tableau_dashboard()


            elif data_source == "ê¸°ë³¸ CSV/Excel" and self.use_default_data:
                if csv_file_exists:
                    df_raw = DataProcessor.load_csv_safe(get_file_path(Config.TBR_DATA_FILES["csv"]))
                    if not df_raw.empty:
                        df_trade = self.process_trade_data(df_raw)
                        if not df_trade.empty:
                            st.success("âœ… SQLite ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìë™ ë¡œë“œ ì™„ë£Œ!")

                            # ì„¹ì…˜ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ í‘œì‹œ
                            st.markdown("---")
                            st.subheader("ğŸ“Š Tableau ëŒ€ì‹œë³´ë“œ")
                            self.render_tableau_dashboard()

                elif excel_file_exists:
                    df_raw = DataProcessor.load_excel_safe(get_file_path(Config.TBR_DATA_FILES["excel"]))
                    if not df_raw.empty:
                        df_trade = self.process_trade_data(df_raw)
                        st.success("âœ… Excel íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ")

            else:  # File upload
                st.markdown("**TBR ê±°ë˜ ë°ì´í„° ì—…ë¡œë“œ**")
                st.caption("ì˜ˆ: trade_data.csv, tbr_market_data.xlsx")
                uploaded_file = st.file_uploader("íŒŒì¼ ì„ íƒ", type=['csv', 'xlsx'], key='tbr_file')

                if uploaded_file:
                    try:
                        if uploaded_file.name.endswith('.csv'):
                            df_raw = DataProcessor.load_csv_from_upload(uploaded_file)
                        else:
                            df_raw = DataProcessor.load_excel_from_upload(uploaded_file)

                        if not df_raw.empty:
                            df_trade = self.process_trade_data(df_raw)
                            st.success("âœ… ì—…ë¡œë“œ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ")
                    except Exception as e:
                        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

        # Display analysis results (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        if not df_trade.empty:
            # Display data info
            st.success(f"âœ… ë°ì´í„° ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ: {len(df_trade):,}í–‰")

            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ì´ ê±°ë˜ê±´ìˆ˜", f"{len(df_trade):,}")
            with col2:
                st.metric("êµ­ê°€ ìˆ˜", df_trade['country'].nunique())
            with col3:
                st.metric("ì—°ë„ ë²”ìœ„", f"{df_trade['year'].min()}-{df_trade['year'].max()}")
            with col4:
                st.metric("ì´ ê±°ë˜ì•¡", format_currency(df_trade['value'].sum(), "USD"))

            # Interactive filters
            st.subheader("ğŸ” ë°ì´í„° í•„í„°")

            # TBR ë¶„ì„ ì„¤ì •ì„ íƒ­ ë‚´ë¶€ì— ë°°ì¹˜
            with st.expander("ğŸ”§ TBR ë¶„ì„ ì„¤ì •", expanded=True):
                col1, col2, col3 = st.columns(3)

                with col1:
                    selected_years = st.multiselect(
                        "ì—°ë„ ì„ íƒ",
                        options=sorted(df_trade['year'].unique()),
                        default=sorted(df_trade['year'].unique())[-3:] if len(df_trade['year'].unique()) >= 3 else sorted(df_trade['year'].unique())
                    )

                with col2:
                    selected_flows = st.multiselect(
                        "ê±°ë˜ ìœ í˜•",
                        options=df_trade['flow'].unique(),
                        default=df_trade['flow'].unique()
                    )

                with col3:
                    selected_countries = st.multiselect(
                        "êµ­ê°€ ì„ íƒ (ë¹ˆ ê°’ = ì „ì²´)",
                        options=sorted(df_trade['country'].unique())
                    )

            # Apply filters
            filtered_df = df_trade[
                (df_trade['year'].isin(selected_years)) &
                (df_trade['flow'].isin(selected_flows))
                ]

            if selected_countries:
                filtered_df = filtered_df[filtered_df['country'].isin(selected_countries)]

            # Visualizations (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
            if not filtered_df.empty:
                # Yearly trend
                st.subheader("ğŸ“Š ì—°ë„ë³„ ê±°ë˜ëŸ‰ ì¶”ì´")
                yearly_data = filtered_df.pivot_table(
                    index='year',
                    columns='flow',
                    values='value',
                    aggfunc='sum'
                ).fillna(0)

                fig = px.line(yearly_data, title="ì—°ë„ë³„ ê±°ë˜ëŸ‰ ì¶”ì´ (USD)")
                fig.update_layout(xaxis_title="ì—°ë„", yaxis_title="ê±°ë˜ì•¡ (USD)")
                st.plotly_chart(fig, use_container_width=True)

                # Top markets
                st.subheader("ğŸ† ì£¼ìš” ì‹œì¥ ë¶„ì„")
                latest_year = filtered_df['year'].max()

                for flow_type in selected_flows:
                    flow_data = filtered_df[
                        (filtered_df['flow'] == flow_type) &
                        (filtered_df['year'] == latest_year)
                        ]

                    if not flow_data.empty:
                        top_markets = (
                            flow_data
                            .groupby('country')['value']
                            .sum()
                            .sort_values(ascending=False)
                            .head(10)
                        )

                        fig = px.bar(
                            x=top_markets.values,
                            y=top_markets.index,
                            orientation='h',
                            title=f"{flow_type} ìƒìœ„ 10ê°œ ì‹œì¥ ({latest_year}ë…„)"
                        )
                        fig.update_layout(xaxis_title="ê±°ë˜ì•¡ (USD)", yaxis_title="êµ­ê°€")
                        st.plotly_chart(fig, use_container_width=True)

                # Data table
                with st.expander("ğŸ“‹ ìƒì„¸ ë°ì´í„°"):
                    st.dataframe(filtered_df.head(100))

            else:
                st.warning("í•„í„° ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        else:
            if not sqlite_file_exists:
                st.info("ğŸ’¡ SQLite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ data í´ë”ì— ì¶”ê°€í•˜ì‹œê±°ë‚˜ ë‹¤ë¥¸ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
# ========================================
# EV TIRE INSIGHT ANALYTICS
# ========================================
class EVTireInsightAnalytics:
    """EV Tire Insight Analytics from Reddit Data"""

    def __init__(self):
        self.logger = setup_logging()
        self.use_default_data = True
        # ì´ 3ì¤„ì„ ì¶”ê°€í•˜ì„¸ìš”
        self.stop_words = None
        self.lemmatizer = None
        self.sia = None
        self.setup_nltk()

    def setup_nltk(self):
        """Setup NLTK resources with error handling"""
        if NLTK_AVAILABLE:
            try:
                nltk.download('stopwords', quiet=True)
                nltk.download('wordnet', quiet=True)
                nltk.download('vader_lexicon', quiet=True)

                self.stop_words = set(stopwords.words("english"))
                self.lemmatizer = WordNetLemmatizer()
                self.sia = SentimentIntensityAnalyzer()

            except Exception as e:
                st.warning(f"NLTK ì„¤ì • ì˜¤ë¥˜: {str(e)}")
                # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
                self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but'])
                self.lemmatizer = None
                self.sia = None
        else:
            # NLTK ì—†ì„ ë•Œ ê¸°ë³¸ê°’
            self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but'])
            self.lemmatizer = None
            self.sia = None

    def collect_reddit_data(self, client_id: str, client_secret: str, user_agent: str,
                            subreddits: List[str], keywords: List[str], limit: int = 500) -> pd.DataFrame:
        """Collect Reddit data using PRAW"""
        if not REDDIT_AVAILABLE:
            st.error("PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `pip install praw tqdm`ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return pd.DataFrame()

        try:
            # Reddit API ì¸ì¦
            reddit = praw.Reddit(
                client_id=client_id,
                client_secret=client_secret,
                user_agent=user_agent
            )
            reddit.read_only = True

            records = []

            # Progress bar setup
            progress_bar = st.progress(0)
            status_text = st.empty()

            total_subreddits = len(subreddits)

            for idx, sub in enumerate(subreddits):
                status_text.text(f"ìˆ˜ì§‘ ì¤‘: r/{sub}")

                try:
                    subreddit = reddit.subreddit(sub)
                    posts = list(subreddit.hot(limit=limit))

                    for post in posts:
                        title = (post.title or "").lower()
                        selftext = (post.selftext or "").lower()

                        # í‚¤ì›Œë“œ ë§¤ì¹˜ ê°œìˆ˜ ì„¸ê¸°
                        match_count = sum(
                            (kw.lower() in title) + (kw.lower() in selftext)
                            for kw in keywords
                        )

                        # ë‘ ê°€ì§€ í‚¤ì›Œë“œ ì´ìƒ í¬í•¨ëœ ê²½ìš°ë§Œ ì €ì¥
                        if match_count >= 2:
                            records.append({
                                'id': post.id,
                                'created_utc': datetime.fromtimestamp(post.created_utc),
                                'subreddit': sub,
                                'author': str(post.author),
                                'title': post.title,
                                'selftext': post.selftext,
                                'score': post.score,
                                'num_comments': post.num_comments,
                                'url': post.url,
                                'matched_keywords': match_count,
                                'type': 'post'
                            })

                        # ëŒ“ê¸€ë„ ìˆ˜ì§‘
                        try:
                            post.comments.replace_more(limit=0)
                            for comment in post.comments.list()[:50]:  # ëŒ“ê¸€ ìˆ˜ ì œí•œ
                                body = (comment.body or "").lower()
                                match_count = sum(kw.lower() in body for kw in keywords)
                                if match_count >= 2:
                                    records.append({
                                        'id': comment.id,
                                        'created_utc': datetime.fromtimestamp(comment.created_utc),
                                        'subreddit': sub,
                                        'parent_id': comment.parent_id,
                                        'author': str(comment.author),
                                        'body': comment.body,
                                        'score': comment.score,
                                        'type': 'comment',
                                        'matched_keywords': match_count
                                    })
                        except Exception as comment_error:
                            st.warning(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜ (r/{sub}): {str(comment_error)}")
                            continue

                except Exception as sub_error:
                    st.error(f"ì„œë¸Œë ˆë”§ r/{sub} ì ‘ê·¼ ì˜¤ë¥˜: {str(sub_error)}")
                    continue

                # Progress update
                progress_bar.progress((idx + 1) / total_subreddits)

            # Clear progress indicators
            progress_bar.empty()
            status_text.empty()

            if records:
                df = pd.DataFrame(records)
                st.success(f"âœ… Reddit ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê±´")
                return df
            else:
                st.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

        except Exception as e:
            st.error(f"Reddit ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame()

    def render_reddit_collector(self):
        """Render Reddit data collection interface"""
        st.subheader("ğŸ” Reddit ë°ì´í„° ìˆ˜ì§‘")
        st.markdown("Reddit APIë¥¼ í†µí•´ EV íƒ€ì´ì–´ ê´€ë ¨ ê²Œì‹œë¬¼ê³¼ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

        if not REDDIT_AVAILABLE:
            st.error("ğŸš¨ PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            st.code("pip install praw tqdm")
            return pd.DataFrame()

        # API ì„¤ì •
        with st.expander("ğŸ”‘ Reddit API ì„¤ì •", expanded=True):
            st.markdown("""
           Reddit API ì‚¬ìš©ì„ ìœ„í•´ [Reddit App](https://www.reddit.com/prefs/apps)ì—ì„œ ì•±ì„ ìƒì„±í•˜ê³  ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.
           """)

            col1, col2 = st.columns(2)
            with col1:
                client_id = st.text_input("Client ID", value="2XU62DdrTzSJUt6Wsy7xuA")
                user_agent = st.text_input("User Agent", value="ev_tire_insights/0.1 by your_username")

            with col2:
                client_secret = st.text_input("Client Secret", type="password", value="FPar49NZEEhGP4C4zL9pdeWXjvnGdA")

        # ìˆ˜ì§‘ ì„¤ì •
        with st.expander("âš™ï¸ ìˆ˜ì§‘ ì„¤ì •", expanded=True):
            col1, col2 = st.columns(2)

            with col1:
                subreddits_input = st.text_area(
                    "ëŒ€ìƒ ì„œë¸Œë ˆë”§ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
                    value="electricvehicles\ntires\nTesla\nEVs"
                )
                subreddits = [s.strip() for s in subreddits_input.split('\n') if s.strip()]

                limit = st.number_input("ì„œë¸Œë ˆë”§ë‹¹ ê²Œì‹œë¬¼ ìˆ˜", min_value=10, max_value=1000, value=500)

            with col2:
                keywords_input = st.text_area(
                    "ê²€ìƒ‰ í‚¤ì›Œë“œ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
                    value="tire\ntyre\nregenerative braking\nnoise\nwear"
                )
                keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]

                min_keywords = st.number_input("ìµœì†Œ í‚¤ì›Œë“œ ë§¤ì¹˜ ìˆ˜", min_value=1, max_value=5, value=2)

        # ë¯¸ë¦¬ë³´ê¸°
        st.info(f"ğŸ¯ ëŒ€ìƒ: {len(subreddits)}ê°œ ì„œë¸Œë ˆë”§, {len(keywords)}ê°œ í‚¤ì›Œë“œ")

        # ìˆ˜ì§‘ ì‹¤í–‰
        collected_data = pd.DataFrame()

        if st.button("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", type="primary"):
            if not client_id or not client_secret or not user_agent:
                st.error("Reddit API ì •ë³´ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                return pd.DataFrame()

            if not subreddits or not keywords:
                st.error("ì„œë¸Œë ˆë”§ê³¼ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                return pd.DataFrame()

            with st.spinner("Reddit ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
                collected_data = self.collect_reddit_data(
                    client_id, client_secret, user_agent,
                    subreddits, keywords, limit
                )

            if not collected_data.empty:
                # ìˆ˜ì§‘ ê²°ê³¼ í‘œì‹œ
                st.subheader("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼")
                col1, col2, col3, col4 = st.columns(4)

                with col1:
                    st.metric("ì´ ìˆ˜ì§‘ ê±´ìˆ˜", len(collected_data))
                with col2:
                    post_count = len(collected_data[collected_data['type'] == 'post']) if 'type' in collected_data.columns else 0
                    st.metric("ê²Œì‹œë¬¼", post_count)
                with col3:
                    comment_count = len(collected_data[collected_data['type'] == 'comment']) if 'type' in collected_data.columns else 0
                    st.metric("ëŒ“ê¸€", comment_count)
                with col4:
                    avg_score = collected_data['score'].mean() if 'score' in collected_data.columns else 0
                    st.metric("í‰ê·  ì ìˆ˜", f"{avg_score:.1f}")

                # ìˆ˜ì§‘ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                with st.expander("ğŸ“‹ ìˆ˜ì§‘ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                    st.dataframe(collected_data.head(10))

                # ë°ì´í„° ì €ì¥ ì˜µì…˜
                if st.button("ğŸ’¾ ë°ì´í„° ì €ì¥"):
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    filename = f"reddit_collected_{timestamp}.csv"
                    filepath = get_file_path(filename)

                    collected_data.to_csv(filepath, index=False, encoding='utf-8-sig')
                    st.success(f"âœ… ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")

        return collected_data

    def clean_text(self, text: str) -> str:
        """Clean text data"""
        if not isinstance(text, str):
            return ""

        text = re.sub(r"http\S+", "", text)  # Remove URLs
        text = re.sub(r"<[^>]+>", "", text)  # Remove HTML tags
        text = re.sub(r"[^\w\s]", " ", text)  # Remove special characters
        text = text.lower().strip()  # Lowercase and trim
        text = re.sub(r"\s+", " ", text)  # Multiple spaces to single
        return text

    def process_reddit_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process Reddit data for analysis"""
        try:
            df_clean = df.copy()

            # Combine text fields
            df_clean["text_raw"] = df_clean.apply(
                lambda r: r["body"] if pd.notna(r.get("body")) and str(r.get("body")).strip()
                else f"{r.get('title', '') or ''} {r.get('selftext', '') or ''}",
                axis=1
            )

            # Clean text
            df_clean["text_clean"] = df_clean["text_raw"].apply(self.clean_text)

            # Parse datetime
            df_clean['created_utc'] = pd.to_datetime(df_clean['created_utc'])

            return df_clean

        except Exception as e:
            st.error(f"Reddit ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame()

    def perform_tfidf_analysis(self, df: pd.DataFrame, max_features: int = 20) -> pd.DataFrame:
        """Perform TF-IDF analysis"""
        try:
            if not NLTK_AVAILABLE:
                st.warning("NLTKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë¶„ì„ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                # Simple word frequency analysis
                all_text = ' '.join(df['text_clean'].fillna(''))
                words = all_text.split()
                word_freq = pd.Series(words).value_counts().head(max_features)
                return pd.DataFrame({
                    'Keyword': word_freq.index,
                    'Frequency': word_freq.values
                })

            vectorizer = TfidfVectorizer(
                max_features=max_features,
                stop_words="english",
                ngram_range=(1, 1)
            )

            tfidf_matrix = vectorizer.fit_transform(df["text_clean"].fillna(''))
            feature_names = vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.sum(axis=0).A1

            tfidf_df = pd.DataFrame({
                'Keyword': feature_names,
                'TF-IDF_Score': tfidf_scores
            }).sort_values('TF-IDF_Score', ascending=False)

            return tfidf_df

        except Exception as e:
            st.error(f"TF-IDF ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame()

    def perform_sentiment_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """Perform sentiment analysis"""
        try:

            if self.sia is None:
                if NLTK_AVAILABLE:
                    try:
                        self.sia = SentimentIntensityAnalyzer()
                    except:
                        pass

            if self.sia is None:
                st.warning("NLTK ê°ì„± ë¶„ì„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°„ë‹¨í•œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                return self.simple_sentiment_fallback(df)

            if not NLTK_AVAILABLE:
                st.warning("NLTKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return df

            # Calculate sentiment scores
            df['sentiment_score'] = df['text_clean'].apply(
                lambda t: self.sia.polarity_scores(t)['compound']
            )

            # Classify sentiment
            df['sentiment'] = df['sentiment_score'].apply(
                lambda s: 'Positive' if s >= Config.SENTIMENT_THRESHOLD_POS
                else ('Negative' if s <= Config.SENTIMENT_THRESHOLD_NEG else 'Neutral')
            )

            return df

        except Exception as e:
            st.error(f"ê°ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return df

    def tokenize_and_lemmatize(self, df: pd.DataFrame) -> pd.DataFrame:
        """Tokenize and lemmatize text"""
        try:
            if not NLTK_AVAILABLE:
                # Simple tokenization
                df["tokens"] = df["text_clean"].str.split()
                return df

            # Tokenize
            df["tokens"] = df["text_clean"].str.split()

            # Remove stop words
            df["tokens_nostop"] = df["tokens"].apply(
                lambda toks: [t for t in toks if t not in self.stop_words]
                if isinstance(toks, list) else []
            )

            # Lemmatize
            df["tokens_lemmatized"] = df["tokens_nostop"].apply(
                lambda toks: [self.lemmatizer.lemmatize(t) for t in toks]
                if isinstance(toks, list) else []
            )

            return df

        except Exception as e:
            st.error(f"í† í°í™” ì˜¤ë¥˜: {str(e)}")
            return df

    def simple_sentiment_fallback(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê°„ë‹¨í•œ ëŒ€ì²´ ê°ì„± ë¶„ì„"""
        df_result = df.copy()

        positive_words = ['good', 'great', 'excellent', 'love', 'best', 'perfect', 'awesome']
        negative_words = ['bad', 'terrible', 'awful', 'hate', 'worst', 'horrible', 'poor']

        def simple_score(text):
            if pd.isna(text):
                return 0.0
            text_lower = str(text).lower()
            pos_count = sum(1 for word in positive_words if word in text_lower)
            neg_count = sum(1 for word in negative_words if word in text_lower)
            return (pos_count - neg_count) * 0.2

        df_result['sentiment_score'] = df_result['text_clean'].apply(simple_score)
        df_result['sentiment'] = df_result['sentiment_score'].apply(
            lambda s: 'Positive' if s > 0.1 else ('Negative' if s < -0.1 else 'Neutral')
        )

        return df_result

    # EVTireInsightAnalytics í´ë˜ìŠ¤
    def reset_state(self):
        """ìƒíƒœ ì´ˆê¸°í™”"""
        self.use_default_data = False
        if 'reddit_df' in st.session_state:
            del st.session_state['reddit_df']

    def render(self):
        """Render EV Tire Insight Analytics UI"""
        st.header("ğŸ”‹ EV Tire Insight Analytics")
        st.markdown("Reddit ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ EV íƒ€ì´ì–´ ê´€ë ¨ ì¸ì‚¬ì´íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

        # Tab selection for data source
        data_tab1, data_tab2 = st.tabs(["ğŸ“Š ë°ì´í„° ë¶„ì„", "ğŸ” ë°ì´í„° ìˆ˜ì§‘"])

        with data_tab2:
            # Reddit data collection interface
            collected_data = self.render_reddit_collector()

            if not collected_data.empty:
                st.markdown("---")
                st.subheader("ğŸ”„ ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ ë¶„ì„ ì§„í–‰")
                if st.button("ìˆ˜ì§‘ëœ ë°ì´í„° ë¶„ì„í•˜ê¸°"):
                    # Switch to analysis with collected data
                    df_processed = self.process_reddit_data(collected_data)
                    if not df_processed.empty:
                        st.success("âœ… ìˆ˜ì§‘ëœ ë°ì´í„° ë¶„ì„ ì™„ë£Œ!")
                        # Continue with analysis...

        with data_tab1:
            # Original analysis interface
            # Check if default data file exists
            reddit_file_exists = check_file_exists(Config.EV_DATA_FILES["reddit"])

            # Data source selection
            col1, col2 = st.columns([3, 1])
            with col1:
                if reddit_file_exists:
                    st.success("âœ… ê¸°ë³¸ Reddit ë°ì´í„° íŒŒì¼ì´ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    self.use_default_data = True
                else:
                    st.info("ğŸ“ ê¸°ë³¸ Reddit ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
                    self.use_default_data = False

            with col2:
                if st.button("ğŸ”„ ì´ˆê¸°í™”", key="ev_reset"):
                    self.use_default_data = False
                    st.rerun()

            # Load data
            df_processed = pd.DataFrame()

            if self.use_default_data and reddit_file_exists:
                # Load default data
                df_raw = DataProcessor.load_csv_safe(get_file_path(Config.EV_DATA_FILES["reddit"]))
                if not df_raw.empty:
                    df_processed = self.process_reddit_data(df_raw)
                    st.info(f"ğŸ“Š ê¸°ë³¸ ë°ì´í„° ì‚¬ìš© ì¤‘: {Config.EV_DATA_FILES['reddit']}")

            else:
                # File upload interface
                st.markdown("**Reddit ë°ì´í„° ì—…ë¡œë“œ**")
                st.caption("ì˜ˆ: ev_tire_reddit_filtered.csv, reddit_comments.csv")
                uploaded_file = st.file_uploader("íŒŒì¼ ì„ íƒ", type=['csv'], key='reddit_file')

                if uploaded_file:
                    df_raw = DataProcessor.load_csv_from_upload(uploaded_file)
                    if not df_raw.empty:
                        df_processed = self.process_reddit_data(df_raw)

            # EV ë¶„ì„ ì„¤ì •ì„ íƒ­ ë‚´ë¶€ì— ë°°ì¹˜
            if not df_processed.empty:
                # 1. ë¨¼ì € ì„¤ì • ë¶€ë¶„
                with st.expander("ğŸ”§ EV ë¶„ì„ ì„¤ì •", expanded=True):
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        max_keywords = st.slider("í‚¤ì›Œë“œ ë¶„ì„ ê°œìˆ˜", 10, 50, 20)
                    with col2:
                        min_score = st.slider("ìµœì†Œ ì ìˆ˜", 0, 100, 1)
                    with col3:
                        analysis_type = st.multiselect(
                            "ë¶„ì„ ìœ í˜• ì„ íƒ",
                            ["í‚¤ì›Œë“œ ë¶„ì„", "ê°ì„± ë¶„ì„", "ì‹œê°„ë³„ íŠ¸ë Œë“œ", "í‚¤ì›Œë“œ íˆíŠ¸ë§µ"],
                            default=["í‚¤ì›Œë“œ ë¶„ì„", "ê°ì„± ë¶„ì„"]
                        )

                # 2. ê·¸ ë‹¤ìŒ ë°ì´í„° ì²˜ë¦¬ ë° ë©”íŠ¸ë¦­ í‘œì‹œ
                try:
                    # Filter by score
                    df_filtered = df_processed[df_processed.get('score', 0) >= min_score]

                    # Display data info
                    st.success(f"âœ… ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(df_filtered):,}ê±´")

                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ì´ ê²Œì‹œë¬¼", len(df_filtered))
                    with col2:
                        st.metric("ì„œë¸Œë ˆë”§", df_filtered['subreddit'].nunique() if 'subreddit' in df_filtered.columns else 0)
                    with col3:
                        st.metric("ê¸°ê°„", f"{df_filtered['created_utc'].dt.date.min()} ~ {df_filtered['created_utc'].dt.date.max()}")
                    with col4:
                        st.metric("í‰ê·  ì ìˆ˜", f"{df_filtered.get('score', pd.Series([0])).mean():.1f}")

                    # 3. ë¶„ì„ ê²°ê³¼ë“¤
                    # Perform selected analyses
                    if "í‚¤ì›Œë“œ ë¶„ì„" in analysis_type:
                        st.subheader("ğŸ” í‚¤ì›Œë“œ ë¶„ì„ (TF-IDF)")

                        tfidf_results = self.perform_tfidf_analysis(df_filtered, max_keywords)

                        if not tfidf_results.empty:
                            col1, col2 = st.columns([2, 1])

                            with col1:
                                # Bar chart
                                fig = px.bar(
                                    tfidf_results.head(15),
                                    x='TF-IDF_Score' if 'TF-IDF_Score' in tfidf_results.columns else 'Frequency',
                                    y='Keyword',
                                    orientation='h',
                                    title="ìƒìœ„ í‚¤ì›Œë“œ"
                                )
                                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                                st.plotly_chart(fig, use_container_width=True)

                            with col2:
                                st.dataframe(tfidf_results.head(15), use_container_width=True)

                    if "ê°ì„± ë¶„ì„" in analysis_type:
                        st.subheader("ğŸ˜Š ê°ì„± ë¶„ì„")

                        df_sentiment = self.perform_sentiment_analysis(df_filtered)

                        if 'sentiment' in df_sentiment.columns:
                            # Sentiment distribution
                            sentiment_counts = df_sentiment['sentiment'].value_counts()

                            col1, col2 = st.columns(2)

                            with col1:
                                fig = px.pie(
                                    values=sentiment_counts.values,
                                    names=sentiment_counts.index,
                                    title="ê°ì„± ë¶„í¬"
                                )
                                st.plotly_chart(fig, use_container_width=True)

                            with col2:
                                st.dataframe(sentiment_counts.to_frame('Count'), use_container_width=True)

                            # Sentiment over time
                            if "ì‹œê°„ë³„ íŠ¸ë Œë“œ" in analysis_type:
                                st.subheader("ğŸ“ˆ ì‹œê°„ë³„ ê°ì„± íŠ¸ë Œë“œ")

                                df_sentiment['year_month'] = df_sentiment['created_utc'].dt.to_period('M')
                                monthly_sentiment = df_sentiment.groupby(['year_month', 'sentiment']).size().unstack(fill_value=0)
                                monthly_sentiment_pct = monthly_sentiment.div(monthly_sentiment.sum(axis=1), axis=0)

                                fig = px.line(
                                    monthly_sentiment_pct.reset_index(),
                                    x='year_month',
                                    y=monthly_sentiment_pct.columns,
                                    title="ì›”ë³„ ê°ì„± ë¹„ìœ¨ ë³€í™”"
                                )
                                st.plotly_chart(fig, use_container_width=True)

                    if "í‚¤ì›Œë“œ íˆíŠ¸ë§µ" in analysis_type and NLTK_AVAILABLE:
                        st.subheader("ğŸ”¥ í‚¤ì›Œë“œ íˆíŠ¸ë§µ")

                        df_tokens = self.tokenize_and_lemmatize(df_filtered)

                        if 'tokens_lemmatized' in df_tokens.columns:
                            # Get top keywords
                            all_tokens = df_tokens['tokens_lemmatized'].explode().dropna()
                            top_keywords = all_tokens.value_counts().head(10).index.tolist()

                            if top_keywords:
                                # Create monthly heatmap data
                                df_expanded = df_tokens.explode('tokens_lemmatized')
                                df_top = df_expanded[df_expanded['tokens_lemmatized'].isin(top_keywords)].copy()
                                df_top['year_month'] = df_top['created_utc'].dt.to_period('M')

                                heatmap_data = (
                                    df_top
                                    .groupby(['year_month', 'tokens_lemmatized'])
                                    .size()
                                    .unstack(fill_value=0)
                                )

                                if not heatmap_data.empty:
                                    fig = px.imshow(
                                        heatmap_data.T,
                                        title="ì›”ë³„ í‚¤ì›Œë“œ ë¹ˆë„ íˆíŠ¸ë§µ",
                                        aspect="auto"
                                    )
                                    st.plotly_chart(fig, use_container_width=True)

                    # Sample data display
                    with st.expander("ğŸ“‹ ìƒ˜í”Œ ë°ì´í„°"):
                        display_cols = ['created_utc', 'subreddit', 'score', 'text_clean']
                        available_cols = [col for col in display_cols if col in df_filtered.columns]
                        st.dataframe(df_filtered[available_cols].head(20))

                    # Download processed data
                    if st.button("ğŸ“¥ ì²˜ë¦¬ëœ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"):
                        csv = df_filtered.to_csv(index=False)
                        st.download_button(
                            label="CSV ë‹¤ìš´ë¡œë“œ",
                            data=csv,
                            file_name=f"processed_reddit_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv"
                        )

                except Exception as e:
                    st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    st.text(traceback.format_exc())

            else:
                if not self.use_default_data:
                    st.info("ğŸ“ Reddit ë°ì´í„° CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ 'ë°ì´í„° ìˆ˜ì§‘' íƒ­ì—ì„œ ìƒˆë¡œ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.")

                # Show expected data format
                with st.expander("ğŸ“‹ ì˜ˆìƒ ë°ì´í„° í˜•ì‹"):
                    sample_data = pd.DataFrame({
                        'id': ['post1', 'post2'],
                        'created_utc': ['2024-01-01 12:00:00', '2024-01-02 13:00:00'],
                        'subreddit': ['electricvehicles', 'tires'],
                        'title': ['EV tire noise issues', 'Best tires for EVs'],
                        'body': ['Content about tire noise...', 'Recommendations for EV tires...'],
                        'score': [15, 23],
                        'num_comments': [5, 8]
                    })
                    st.dataframe(sample_data)

# ========================================
# APPLICATION ENTRY POINT
# ========================================
def main():
    """Main application entry point"""
    # Configure Streamlit
    st.set_page_config(
        page_title=Config.APP_TITLE,
        page_icon="ğŸš€",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # Ensure data folder exists
    ensure_data_folder()

    # Main title and description
    st.title(Config.APP_TITLE)
    st.markdown(f"**Version {Config.VERSION}** | í†µí•© ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

    # Show data folder status
    with st.expander("ğŸ“ ë°ì´í„° í´ë” ìƒíƒœ"):
        st.markdown(f"**ë°ì´í„° í´ë” ê²½ë¡œ:** `{Config.DATA_FOLDER}`")

        # Fleet TCO files
        st.markdown("**Fleet TCO íŒŒì¼:**")
        for key, filename in Config.FLEET_DATA_FILES.items():
            exists = check_file_exists(filename)
            status = "âœ…" if exists else "âŒ"
            st.markdown(f"- {status} {filename}")

        # TBR files
        st.markdown("**TBR Market íŒŒì¼:**")
        for key, filename in Config.TBR_DATA_FILES.items():
            exists = check_file_exists(filename)
            status = "âœ…" if exists else "âŒ"
            st.markdown(f"- {status} {filename}")

        # EV files
        st.markdown("**EV Analytics íŒŒì¼:**")
        for key, filename in Config.EV_DATA_FILES.items():
            exists = check_file_exists(filename)
            status = "âœ…" if exists else "âŒ"
            st.markdown(f"- {status} {filename}")

    # Simplified sidebar - ê³µí†µ ì •ë³´ë§Œ
    with st.sidebar:
        st.header("ğŸ“Š ëŒ€ì‹œë³´ë“œ ì •ë³´")
        st.markdown("""
        ### í¬í•¨ëœ ë¶„ì„ ë„êµ¬:
        - **Fleet TCO Calculator**: ì°¨ëŸ‰ ìš´ì˜ ì´ë¹„ìš© ê³„ì‚°
        - **TBR Market Dashboard**: ê¸€ë¡œë²Œ TBR ì‹œì¥ ë¶„ì„
        - **EV Tire Analytics**: EV íƒ€ì´ì–´ ì¸ì‚¬ì´íŠ¸ ë¶„ì„
        
        ### ì§€ì› íŒŒì¼ í˜•ì‹:
        - CSV (UTF-8, UTF-8-BOM, CP949, EUC-KR)
        - Excel (XLSX, XLS)
        - SQLite (TBR ë¶„ì„ìš©)
        
        ### ê¸°ë³¸ ë°ì´í„° íŒŒì¼:
        ë°ì´í„° í´ë”ì— í•´ë‹¹ ì´ë¦„ì˜ íŒŒì¼ì„ ì €ì¥í•˜ë©´ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.
        """)

        # Library status
        st.markdown("### ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ:")
        if NLTK_AVAILABLE:
            st.success("âœ… NLTK ì„¤ì¹˜ë¨")
        else:
            st.error("âŒ NLTK ë¯¸ì„¤ì¹˜")
            st.code("pip install nltk scikit-learn")

        if REDDIT_AVAILABLE:
            st.success("âœ… PRAW ì„¤ì¹˜ë¨")
        else:
            st.error("âŒ PRAW ë¯¸ì„¤ì¹˜")
            st.code("pip install praw tqdm")

    # Main content area with tabs
    tab1, tab2, tab3 = st.tabs([
        "ğŸš› Fleet TCO Calculator",
        "ğŸŒ TBR Market Dashboard",
        "ğŸ”‹ EV Tire Analytics"
    ])

    with tab1:
        try:
            calculator = FleetTCOCalculator()
            calculator.render()
        except Exception as e:
            st.error(f"Fleet TCO Calculator ì˜¤ë¥˜: {str(e)}")

    with tab2:
        try:
            dashboard = TBRMarketDashboard()
            dashboard.render()
        except Exception as e:
            st.error(f"TBR Market Dashboard ì˜¤ë¥˜: {str(e)}")

    with tab3:
        try:
            analytics = EVTireInsightAnalytics()
            analytics.render()
        except Exception as e:
            st.error(f"EV Tire Analytics ì˜¤ë¥˜: {str(e)}")

    # Footer
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666; font-size: 0.8em;'>
        ğŸš€ Integrated Analytics Dashboard | 
        Built with Streamlit | 
        Data-driven insights for transportation industry
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()  # ì´ ë¶€ë¶„ì´ ìˆëŠ”ì§€ í™•ì¸!
